{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383cfd2-4fe3-4840-8b3f-7c5363f543ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 1\n",
    "    \n",
    "In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) \n",
    "and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over \n",
    "polynomials of the original variables, allowing learning of non-linear models.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123a031-c5cb-4b62-a2a7-4b388b476e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 2\n",
    "    \n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the SVM classifier with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)  # Adjust degree as needed\n",
    "svm_poly.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_poly.predict(X_test_std)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of SVM with polynomial kernel:\", accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6133eee-08ee-4f07-9854-9fbbb5fc21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 3\n",
    "    \n",
    "In Support Vector Regression (SVR), epsilon (\n",
    "ùúñ\n",
    "œµ) is a hyperparameter that controls the width of the margin around the predicted function within which no penalty is associated with the training points. It essentially defines a tube around the regression line within which errors are not penalized.\n",
    "\n",
    "When you increase the value of epsilon in SVR, you are widening this tube, allowing more data points to fall within the margin. As a result:\n",
    "\n",
    "1.More support vectors: With a wider margin, more data points are allowed to be within the margin, and these data points become support vectors. These are the data points that are either on the margin boundary or inside the margin.\n",
    "\n",
    "2.Smoothing of the regression function: A larger epsilon encourages a smoother regression function, as it allows the model to focus less on fitting individual data points exactly and more on capturing the overall trend of the data.\n",
    "\n",
    "3.Increased generalization: By allowing more data points to fall within the margin, the model may generalize better to unseen data, as it is less sensitive to individual training instances.\n",
    "However, it's essential to note that increasing epsilon too much can lead to underfitting, as the model becomes overly simplified and may fail to capture the complexity of the underlying data.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR tends to increase the number of support vectors and promote a smoother, more generalized regression function, but it's essential to find the right balance to avoid underfitting.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d65284-3646-40d4-b2e2-f0254294cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 4\n",
    "    \n",
    "1.Kernel Function:\n",
    "The kernel function determines the type of mapping applied to the input features. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "Example: Use a polynomial kernel when the relationship between input features and output is believed to be polynomial. Use an RBF kernel when the relationship is non-linear and potentially complex.\n",
    "\n",
    "2.C Parameter:\n",
    "The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A smaller C leads to a softer margin (more misclassifications allowed), while a larger C leads to a harder margin (fewer misclassifications allowed).\n",
    "Example: Increase C when the training data is noisy or overlapping, as it helps in fitting the training data more accurately. Decrease C when you want to encourage a smoother decision boundary or when overfitting is a concern.\n",
    "\n",
    "3.Epsilon Parameter:\n",
    "The epsilon parameter (\n",
    "ùúñ\n",
    "œµ) determines the width of the margin around the regression function within which no penalty is associated with training points. It defines a tube around the regression line within which errors are not penalized.\n",
    "Example: Increase epsilon when you want to allow more flexibility in fitting the data, especially if the true relationship between features and output is believed to have some uncertainty. Decrease epsilon for a tighter fit to the training data and when you want to penalize errors more strictly.\n",
    "\n",
    "4.Gamma Parameter:\n",
    "The gamma parameter (\n",
    "ùõæ\n",
    "Œ≥) defines the influence of a single training example, with low values meaning ‚Äòfar‚Äô and high values meaning ‚Äòclose‚Äô. Higher values of gamma lead to more complex decision boundaries.\n",
    "Example: Increase gamma when you want the model to focus more on local data points and capture finer details of the training data, especially in non-linear problems with complex decision boundaries. Decrease gamma when you want to prevent overfitting and encourage the model to consider a broader range of data points.\n",
    "In summary, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter significantly impacts the performance of SVR. It's essential to understand the characteristics of each parameter and how they interact with the data to make informed decisions when tuning them for a particular problem. Experimentation and validation with cross-validation techniques are crucial for finding the optimal values for these parameters.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec8456-805a-433a-ab13-48c9d4391312",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 5\n",
    "    \n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming you have a dataset named \"data.csv\"\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = data.drop(columns=[\"target_column\"])  # Replace \"target_column\" with the actual target column name\n",
    "y = data[\"target_column\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best estimator:\")\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'trained_classifier.pkl')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
